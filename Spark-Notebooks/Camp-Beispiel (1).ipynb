{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "mySparkPool",
              "session_id": "0",
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-10-02T06:08:00.8873787Z",
              "session_start_time": "2023-10-02T06:08:00.9696685Z",
              "execution_start_time": "2023-10-02T06:10:59.0275099Z",
              "execution_finish_time": "2023-10-02T06:11:10.5264019Z",
              "spark_jobs": null,
              "parent_msg_id": "62813bca-32bd-4163-bc80-1bfc43417460"
            },
            "text/plain": "StatementMeta(mySparkPool, 0, 2, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo\n  Downloading pymongo-4.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m671.3/671.3 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting dnspython<3.0.0,>=1.16.0\n  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: dnspython, pymongo\nSuccessfully installed dnspython-2.4.2 pymongo-4.5.0\nNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "mySparkPool",
              "session_id": "12",
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-10-05T17:53:47.2009129Z",
              "session_start_time": "2023-10-05T17:53:47.275161Z",
              "execution_start_time": "2023-10-05T17:56:51.6265039Z",
              "execution_finish_time": "2023-10-05T17:56:58.7891317Z",
              "spark_jobs": null,
              "parent_msg_id": "c5df752a-e995-412a-b191-19be1d37b6c1"
            },
            "text/plain": "StatementMeta(mySparkPool, 12, 2, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from pyspark.sql.functions import col, explode,current_date, datediff,from_unixtime, expr, avg\r\n",
        "from pyspark.sql.types import IntegerType\r\n",
        "from datetime import datetime\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "mySparkPool",
              "session_id": "12",
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-10-05T17:57:18.5870274Z",
              "session_start_time": null,
              "execution_start_time": "2023-10-05T17:57:18.7775145Z",
              "execution_finish_time": "2023-10-05T17:57:38.2239618Z",
              "spark_jobs": null,
              "parent_msg_id": "1cda51e9-fa7e-4592-9777-495a17dc9681"
            },
            "text/plain": "StatementMeta(mySparkPool, 12, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n |-- _id: struct (nullable = true)\n |    |-- $oid: string (nullable = true)\n |-- account_id: struct (nullable = true)\n |    |-- $numberInt: string (nullable = true)\n |-- limit: struct (nullable = true)\n |    |-- $numberInt: string (nullable = true)\n |-- products: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\nroot\n |-- account_id: string (nullable = true)\n |-- limit: string (nullable = true)\n |-- product: string (nullable = true)\n\n"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      },
      "source": [
        "\r\n",
        "%%pyspark\r\n",
        "df_accounts = spark.read.load('abfss://synapse@jgastorage.dfs.core.windows.net/accounts.json', format='json')\r\n",
        "\r\n",
        "df_accounts.printSchema()\r\n",
        "# ?? WHich is the most used Service ?? #\r\n",
        "\r\n",
        "# unnesting the products array\r\n",
        "df_exploded = df_accounts.select(\"account_id\", \"limit\", explode(col(\"products\")).alias(\"product\"))\r\n",
        "\r\n",
        "df_exploded = df_exploded.select(col(\"account_id.$numberInt\").alias(\"account_id\"), col(\"limit.$numberInt\").alias(\"limit\"), \"product\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "df_exploded.printSchema()\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "mySparkPool",
              "session_id": "5",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-10-04T09:06:58.328192Z",
              "session_start_time": null,
              "execution_start_time": "2023-10-04T09:06:58.5555632Z",
              "execution_finish_time": "2023-10-04T09:07:04.3651186Z",
              "spark_jobs": null,
              "parent_msg_id": "c055f2f1-5292-4a6a-881e-387658f44c63"
            },
            "text/plain": "StatementMeta(mySparkPool, 5, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "df_exploded.write.mode(\"overwrite\").saveAsTable(\"accounts\")\r\n",
        "\r\n",
        "df_exploded.write \\\r\n",
        "    .format(\"com.databricks.spark.csv\") \\\r\n",
        "    .option(\"header\", \"true\") \\\r\n",
        "    .mode(\"overwrite\") \\\r\n",
        "    .save('abfss://synapse@jgastorage.dfs.core.windows.net/accounts.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "mySparkPool",
              "session_id": "12",
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-10-05T17:58:07.4242607Z",
              "session_start_time": null,
              "execution_start_time": "2023-10-05T17:58:07.635335Z",
              "execution_finish_time": "2023-10-05T17:58:50.2744637Z",
              "spark_jobs": null,
              "parent_msg_id": "9bf68f41-8c3c-4860-8efa-94e8aa8793d6"
            },
            "text/plain": "StatementMeta(mySparkPool, 12, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+------+--------------------+----------------+--------------------+-----------------+--------------------+----------------+\n|                 _id|            accounts|active|             address|       birthdate|               email|             name|    tier_and_details|        username|\n+--------------------+--------------------+------+--------------------+----------------+--------------------+-----------------+--------------------+----------------+\n|{5ca4bbcea2dd94ee...|[{371138}, {32428...|  true|9286 Bethany Glen...|{{226117231000}}|arroyocolton@gmai...|    Elizabeth Ray|{null, null, null...|         fmiller|\n|{5ca4bbcea2dd94ee...|          [{116508}]|  null|Unit 1047 Box 408...|{{761701587000}}|cooperalexis@hotm...|    Lindsay Cowan|{null, null, null...|valenciajennifer|\n|{5ca4bbcea2dd94ee...|[{462501}, {22829...|  null|55711 Janet Plaza...|{{582848134000}}|timothy78@hotmail...|  Katherine David|{null, null, null...|      hillrachel|\n|{5ca4bbcea2dd94ee...|[{170945}, {951849}]|  null|Unit 2676 Box 935...|{{154708220000}}| tcrawford@gmail.com|  Leslie Martinez|{null, null, null...|    serranobrian|\n|{5ca4bbcea2dd94ee...|[{721914}, {81722...|  null|2765 Powers Meado...|{{231803855000}}|  dustin37@yahoo.com|    Brad Cardenas|{null, null, null...|   charleshudson|\n|{5ca4bbcea2dd94ee...|[{904260}, {565468}]|  null|17677 Mark Crest\\...|{{842634867000}}|amyholland@yahoo.com|     Natalie Ford|{null, null, null...| gregoryharrison|\n|{5ca4bbcea2dd94ee...|[{627629}, {55958...|  null|50047 Smith Point...|{{-16752040000}}| vcarter@hotmail.com|      Dana Clarke|{null, null, null...|          hmyers|\n|{5ca4bbcea2dd94ee...|[{385397}, {33797...|  null|633 Miller Turnpi...|{{730661849000}}|   laura34@yahoo.com|     Gary Nichols|{null, null, null...|  andrewhamilton|\n|{5ca4bbcea2dd94ee...|[{702610}, {240640}]|  null|38456 Rachael Cau...|{{732022654000}}|   zmelton@gmail.com|       John Parks|{null, null, null...|      matthewray|\n|{5ca4bbcea2dd94ee...|[{344885}, {83992...|  null|4140 Pamela Hollo...| {{90241268000}}|   scott50@yahoo.com|Jennifer Lawrence|{null, null, null...|          glopez|\n|{5ca4bbcea2dd94ee...|          [{987709}]|  null|8681 Karen Roads ...| {{95789846000}}|josephmacias@hotm...|    James Sanchez|{null, null, null...|        wesley20|\n|{5ca4bbcea2dd94ee...|[{662207}, {816481}]|  null|18637 Jessica Rid...|{{627927174000}}|michael16@hotmail...|     Ashley Lopez|{null, null, null...|     thomasdavid|\n|{5ca4bbcea2dd94ee...|          [{571880}]|  null|2129 Joel Rapids\\...|{{235600552000}}|michaelespinoza@g...| Dr. Angela Brown|{null, null, null...|      patricia44|\n|{5ca4bbcea2dd94ee...|[{88112}, {567199...|  null|86636 Maria Viadu...|{{432962538000}}|  ryanpena@yahoo.com|        John Vega|{null, null, null...|     nelsonmaria|\n|{5ca4bbcea2dd94ee...|[{883283}, {98086...|  null|1579 Young Trail\\...|{{341598359000}}|briannafrost@yaho...|     Lauren Clark|{null, null, null...|   portermichael|\n|{5ca4bbcea2dd94ee...|[{631901}, {814687}]|  null|USNS Howard\\nFPO ...|{{399712377000}}|virginia36@hotmai...|Jacqueline Haynes|{null, null, null...|   johnsonshelly|\n|{5ca4bbcea2dd94ee...|[{550665}, {321695}]|  null|70092 Adams Prair...|{{286857767000}}|   april04@gmail.com|     Brian Flores|{null, null, null...|    hunterdaniel|\n|{5ca4bbcea2dd94ee...|[{66698}, {859246...|  null|7322 Owens Inlet ...|{{156860840000}}|   omolina@gmail.com|Christopher Gomez|{null, null, null...|         james75|\n|{5ca4bbcea2dd94ee...|[{205563}, {61660...|  null|86176 Katherine C...|{{648222432000}}|barbaraduncan@gma...|     Robert Burns|{null, null, null...|          eric10|\n|{5ca4bbcea2dd94ee...|[{700880}, {37684...|  null|932 Jeremy Spring...|{{363566100000}}|nicoleanderson@ho...|    Joshua Parker|{null, null, null...|     millerrenee|\n+--------------------+--------------------+------+--------------------+----------------+--------------------+-----------------+--------------------+----------------+\nonly showing top 20 rows\n\nroot\n |-- username: string (nullable = true)\n |-- name: string (nullable = true)\n |-- address: string (nullable = true)\n |-- Time: long (nullable = true)\n |-- account_id: string (nullable = true)\n |-- date_of_birth: string (nullable = true)\n |-- age: double (nullable = true)\n\n+----------------+---------------+--------------------+------------+----------+-------------+------------------+\n|        username|           name|             address|        Time|account_id|date_of_birth|               age|\n+----------------+---------------+--------------------+------------+----------+-------------+------------------+\n|         fmiller|  Elizabeth Ray|9286 Bethany Glen...|226117231000|    371138|   1977-03-02| 46.62465753424657|\n|         fmiller|  Elizabeth Ray|9286 Bethany Glen...|226117231000|    324287|   1977-03-02| 46.62465753424657|\n|         fmiller|  Elizabeth Ray|9286 Bethany Glen...|226117231000|    276528|   1977-03-02| 46.62465753424657|\n|         fmiller|  Elizabeth Ray|9286 Bethany Glen...|226117231000|    332179|   1977-03-02| 46.62465753424657|\n|         fmiller|  Elizabeth Ray|9286 Bethany Glen...|226117231000|    422649|   1977-03-02| 46.62465753424657|\n|         fmiller|  Elizabeth Ray|9286 Bethany Glen...|226117231000|    387979|   1977-03-02| 46.62465753424657|\n|valenciajennifer|  Lindsay Cowan|Unit 1047 Box 408...|761701587000|    116508|   1994-02-19|29.643835616438356|\n|      hillrachel|Katherine David|55711 Janet Plaza...|582848134000|    462501|   1988-06-20| 35.31506849315068|\n|      hillrachel|Katherine David|55711 Janet Plaza...|582848134000|    228290|   1988-06-20| 35.31506849315068|\n|      hillrachel|Katherine David|55711 Janet Plaza...|582848134000|    968786|   1988-06-20| 35.31506849315068|\n|      hillrachel|Katherine David|55711 Janet Plaza...|582848134000|    515844|   1988-06-20| 35.31506849315068|\n|      hillrachel|Katherine David|55711 Janet Plaza...|582848134000|    377292|   1988-06-20| 35.31506849315068|\n|    serranobrian|Leslie Martinez|Unit 2676 Box 935...|154708220000|    170945|   1974-11-26| 48.89041095890411|\n|    serranobrian|Leslie Martinez|Unit 2676 Box 935...|154708220000|    951849|   1974-11-26| 48.89041095890411|\n|   charleshudson|  Brad Cardenas|2765 Powers Meado...|231803855000|    721914|   1977-05-06|46.446575342465756|\n|   charleshudson|  Brad Cardenas|2765 Powers Meado...|231803855000|    817222|   1977-05-06|46.446575342465756|\n|   charleshudson|  Brad Cardenas|2765 Powers Meado...|231803855000|    973067|   1977-05-06|46.446575342465756|\n|   charleshudson|  Brad Cardenas|2765 Powers Meado...|231803855000|    260799|   1977-05-06|46.446575342465756|\n|   charleshudson|  Brad Cardenas|2765 Powers Meado...|231803855000|     87389|   1977-05-06|46.446575342465756|\n| gregoryharrison|   Natalie Ford|17677 Mark Crest\\...|842634867000|    904260|   1996-09-13|27.076712328767123|\n+----------------+---------------+--------------------+------------+----------+-------------+------------------+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "metadata": {
        "microsoft": {
          "language": "python"
        }
      },
      "source": [
        "\r\n",
        "%%pyspark\r\n",
        "df_customers = spark.read.load('abfss://synapse@jgastorage.dfs.core.windows.net/customers.json', format='json')\r\n",
        "\r\n",
        "df_customers.show()\r\n",
        "\r\n",
        "import math\r\n",
        "\r\n",
        "#def calculate_age(seconds):\r\n",
        "    #birthdate = datetime.fromtimestamp(seconds)\r\n",
        "    #current_date = datetime.now()\r\n",
        "   # age = current_date.year - birthdate.year - ((current_date.month, current_date.day) < (birthdate.month, birthdate.day))\r\n",
        "    #return age\r\n",
        "\r\n",
        "# ?? Wie viele Accounts hat jeder Kunde ?? #\r\n",
        "\r\n",
        "\r\n",
        "def unix_timestamp_to_date(unix_timestamp):\r\n",
        "    a = unix_timestamp / 1000\r\n",
        "    return from_unixtime(a, \"yyyy-MM-dd\")\r\n",
        "\r\n",
        "#calculate_age_udf = spark.udf.register(\"calculate_age\", calculate_age, IntegerType())\r\n",
        "\r\n",
        "spark.udf.register(\"unix_timestamp_to_date\", unix_timestamp_to_date)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "df_neu = df_customers.select(\"username\", \"name\", \"address\",  col(\"birthdate.$date\").alias(\"Time\") , explode(col(\"accounts\")).alias(\"account\") )\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "df_ohne = df_neu.select(\"username\", \"name\", \"address\", col(\"Time.$numberLong\").alias(\"Time\"), col(\"account.$numberInt\").alias(\"account_id\"))\r\n",
        "\r\n",
        "df_ohne = df_ohne.withColumn(\"Time\", col(\"Time\").cast(\"bigint\"))\r\n",
        "\r\n",
        "df_ohne = df_ohne.withColumn(\"date_of_birth\", unix_timestamp_to_date(col(\"Time\")))\r\n",
        "df_fertig = df_ohne.withColumn(\"age\", datediff(current_date(), col(\"date_of_birth\")) / 365)\r\n",
        "\r\n",
        "#df_fertig = df_ohne.withColumn(\"age\", calculate_age_udf(col(\"Time\")) )\r\n",
        "\r\n",
        "df_fertig.printSchema()\r\n",
        "\r\n",
        "#df_fertig.write.mode(\"overwrite\").saveAsTable(\"customers\")\r\n",
        "\r\n",
        "df_fertig.show()\r\n",
        "\r\n",
        "\r\n",
        "df_fertig.write.mode(\"overwrite\").saveAsTable(\"customers\")\r\n",
        "\r\n",
        "df_fertig.write \\\r\n",
        "    .format(\"com.databricks.spark.csv\") \\\r\n",
        "    .option(\"header\", \"true\") \\\r\n",
        "    .mode(\"overwrite\") \\\r\n",
        "    .save('abfss://synapse@jgastorage.dfs.core.windows.net/customers.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "mySparkPool",
              "session_id": "5",
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-10-04T09:01:16.593865Z",
              "session_start_time": null,
              "execution_start_time": "2023-10-04T09:01:16.8285949Z",
              "execution_finish_time": "2023-10-04T09:01:36.5093339Z",
              "spark_jobs": null,
              "parent_msg_id": "ff53d671-c59d-4c5e-9f31-484363955aae"
            },
            "text/plain": "StatementMeta(mySparkPool, 5, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n |-- account_id: string (nullable = true)\n |-- end_date: string (nullable = true)\n |-- start_date: string (nullable = true)\n |-- transaction_count: string (nullable = true)\n |-- bucket_time: double (nullable = true)\n |-- transaction_code: string (nullable = true)\n |-- transaction_date: string (nullable = true)\n\n+----------+----------+----------+-----------------+------------------+----------------+----------------+\n|account_id|  end_date|start_date|transaction_count|       bucket_time|transaction_code|transaction_date|\n+----------+----------+----------+-----------------+------------------+----------------+----------------+\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|             buy|      2003-09-09|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|             buy|      2016-06-14|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|             buy|      2002-12-04|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|            sell|      2014-07-14|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|             buy|      2011-10-28|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|            sell|      2010-12-20|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|            sell|      2015-10-28|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|             buy|      2012-02-02|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|            sell|      2016-03-14|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|             buy|      2012-03-01|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|             buy|      2001-01-04|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|             buy|      2015-12-14|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|             buy|      2016-10-05|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|             buy|      2016-09-26|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|            sell|      1992-07-17|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|             buy|      1978-02-16|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|            sell|      2016-06-01|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|            sell|      1986-10-23|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|             buy|      2010-10-22|\n|    443178|2017-01-03|1969-02-04|               66|47.945205479452056|            sell|      2016-07-28|\n+----------+----------+----------+-----------------+------------------+----------------+----------------+\nonly showing top 20 rows\n\n"
          ]
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        }
      },
      "source": [
        "%%pyspark\r\n",
        "df_transactions = spark.read.load('abfss://synapse@jgastorage.dfs.core.windows.net/transactions.json', format='json')\r\n",
        "\r\n",
        "\r\n",
        "def unix_timestamp_to_date(unix_timestamp):\r\n",
        "    a = unix_timestamp / 1000\r\n",
        "    return from_unixtime(a, \"yyyy-MM-dd\")\r\n",
        "\r\n",
        "spark.udf.register(\"unix_timestamp_to_date\", unix_timestamp_to_date)\r\n",
        "\r\n",
        "df_neu = df_transactions.select(col(\"account_id.$numberInt\").alias(\"account_id\"), col(\"bucket_end_date.$date.$numberLong\").alias(\"end_date\"), col(\"bucket_start_date.$date.$numberLong\").alias(\"start_date\"),\r\n",
        "col(\"transaction_count.$numberInt\").alias(\"transaction_count\") , explode(col(\"transactions\")).alias(\"transactions\")        )\r\n",
        "\r\n",
        "\r\n",
        "# calculating the lenght of the timebucket\r\n",
        "df_neu = df_neu.withColumn(\"start_date\", unix_timestamp_to_date(col(\"start_date\")))\r\n",
        "\r\n",
        "df_neu = df_neu.withColumn(\"end_date\", unix_timestamp_to_date(col(\"end_date\")))\r\n",
        "\r\n",
        "df_neu = df_neu.withColumn(\"bucket_time\", datediff(col(\"end_date\"), col(\"start_date\"))/365)\r\n",
        "\r\n",
        "\r\n",
        "# now extracting the transaction_code and date\r\n",
        "\r\n",
        "df_neu = df_neu.withColumn(\"transaction_code\", col(\"transactions.transaction_code\"))\r\n",
        "df_neu = df_neu.withColumn(\"transaction_date\", col(\"transactions.date.$date.$numberLong\"))\r\n",
        "\r\n",
        "# converting unix timestamp to date\r\n",
        "df_neu = df_neu.withColumn(\"transaction_date\", unix_timestamp_to_date(col(\"transaction_date\")))\r\n",
        "\r\n",
        "# now only select the necessary columns\r\n",
        "\r\n",
        "df_neu = df_neu.select(\"account_id\", \"end_date\", \"start_date\", \"transaction_count\", \"bucket_time\", \"transaction_code\", \"transaction_date\")\r\n",
        "\r\n",
        "df_neu.printSchema()\r\n",
        "df_neu.show()\r\n",
        "\r\n",
        "# now save as table\r\n",
        "\r\n",
        "df_neu.write.mode(\"overwrite\").saveAsTable(\"transactions\")\r\n",
        "\r\n",
        "df_neu.write \\\r\n",
        "    .format(\"com.databricks.spark.csv\") \\\r\n",
        "    .option(\"header\", \"true\") \\\r\n",
        "    .mode(\"overwrite\") \\\r\n",
        "    .save('abfss://synapse@jgastorage.dfs.core.windows.net/transactions.csv')\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# now vizualize the data\r\n",
        "\r\n",
        "# Frage: Welcher Account typy ist der häufigste"
      ]
    }
  ]
}